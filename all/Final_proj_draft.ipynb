{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Shawn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## packages\n",
    "import collections\n",
    "import numpy as np\n",
    "import textblob as tb\n",
    "from textblob import TextBlob as TB\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from scipy.spatial import distance\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "import pprint as pp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressors\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.linear_model import Lasso\n",
    "# from sklearn.linear_model import ElasticNet\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random split to 8000:2000 (train:validate)\n",
    "# training_indice = np.random.choice(10000, size=8000, replace=False)\n",
    "# validation_indice = [item for item in range(10000) if item not in training_indice]\n",
    "\n",
    "## fix split to 8000:2000 (train:validate)\n",
    "train_indice = range(10000)\n",
    "# validate_indice = range(8000,10000)\n",
    "test_indice = range(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reform each tag file into a dictionary(key: category, value: tags array), collect all these dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## translate all tag_file into a dictionary and collect them into an array\n",
    "def tag_reform_collection(source,indice):\n",
    "    tags_records = []\n",
    "    for i in indice:\n",
    "        current_dict = collections.defaultdict(list)\n",
    "        path = source + str(i) +\".txt\"\n",
    "        fo = open(path, \"r\")\n",
    "        for line in fo.readlines():\n",
    "            line = line.strip()\n",
    "            current=line.split(\":\")[0]\n",
    "            current_dict[current].append(line.split(\":\")[1])\n",
    "        tags_records.append(dict(current_dict))\n",
    "    return tags_records\n",
    "        \n",
    "train_tags_records = tag_reform_collection(\"data/tags_train/\",train_indice)\n",
    "# validate_tags_records = tag_reform_collection(\"data/tags_train/\",validate_indice)\n",
    "test_tags_records = tag_reform_collection(\"data/tags_test/\",test_indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output\n",
    "# pp.pprint(train_tags_records[0:2])\n",
    "# pp.pprint(validate_tags_records[0:2])\n",
    "# pp.pprint(test_tags_records[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a set of categories (12 categories) for training, validation, and testing set (should be same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cate_set(source,indice):\n",
    "    cate_set = set([])\n",
    "    for i in indice:\n",
    "        path = source + str(i) +\".txt\"\n",
    "        fo = open(path, \"r\")\n",
    "        for line in fo.readlines():\n",
    "            line = line.strip()\n",
    "            cate_set.add(line.split(\":\")[0])\n",
    "    return cate_set\n",
    "\n",
    "train_cate_set = cate_set(\"data/tags_train/\",train_indice)\n",
    "# validate_cate_set = cate_set(\"data/tags_train/\",validate_indice)\n",
    "test_cate_set = cate_set(\"data/tags_test/\",test_indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output\n",
    "# pp.pprint(train_cate_set)\n",
    "# pp.pprint(validate_cate_set)\n",
    "# pp.pprint(test_cate_set)\n",
    "\n",
    "# these category set comes to be exactly the same!\n",
    "cate_set = train_cate_set\n",
    "# pp.pprint(cate_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build category-tags dictionary in training, validation, and testing set (should be same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cate_tags_dictionary(indice, tags_records):\n",
    "    tags_dic = {}\n",
    "    for cate in cate_set:\n",
    "        tags=set()\n",
    "        for i in range(len(indice)):\n",
    "            if cate in tags_records[i]:\n",
    "                tags=tags.union(tags_records[i][cate])\n",
    "        tags=list(tags)\n",
    "        tags.sort()\n",
    "        tags_dic[cate] = tags\n",
    "    return tags_dic\n",
    "\n",
    "train_tags_dic = cate_tags_dictionary(train_indice, train_tags_records)\n",
    "# validate_tags_dic = cate_tags_dictionary(validate_indice, validate_tags_records)\n",
    "test_tags_dic = cate_tags_dictionary(test_indice, test_tags_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train_tags_dic, validate_tags_dic, and test_tags_dic are the same!\n",
    "tags_dic = train_tags_dic\n",
    "# tags_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Building labels for each categories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_labels(indice,tags_records):\n",
    "    image_labels = dict([(key, []) for key in tags_dic])\n",
    "    for i in range(len(indice)):\n",
    "        for k in tags_dic.keys():\n",
    "            label=[0 for d in range(len(tags_dic[k]))]\n",
    "            if k in tags_records[i]:\n",
    "                for index,elt in enumerate(tags_dic[k]):\n",
    "                    if elt in tags_records[i][k]:\n",
    "                        label[index]=1\n",
    "            image_labels[k].append(label)\n",
    "    return image_labels\n",
    "            \n",
    "train_image_labels = build_labels(train_indice,train_tags_records)\n",
    "# validate_image_labels = build_labels(validate_indice,validate_tags_records)\n",
    "test_image_labels = build_labels(test_indice,test_tags_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All label-matrix for all tag-categories (ready for train)\n",
    "# train_image_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Extracting features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1ï¼‰ Build feature space (and count the word frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 3.47 s, total: 1min 9s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "total_count = collections.defaultdict(int)\n",
    "\n",
    "for i in train_indice:\n",
    "    path = \"data/descriptions_train/\" + str(i) + \".txt\"\n",
    "    fo = open(path, \"r\")\n",
    "    for line in fo.readlines():\n",
    "        line = line.strip()\n",
    "        ret = TB(line).lower()\n",
    "        \n",
    "        sentence_nouns=[]\n",
    "        sentence_adjs=[]\n",
    "        for word,pos in ret.tags:\n",
    "            if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS') and word not in stopwords:\n",
    "                sentence_nouns.append(tb.blob.Word(word).singularize())\n",
    "#             if (pos == 'JJ' or pos == 'JJR' or pos == 'JJS') and word not in stopwords:\n",
    "#                 sentence_adjs.append(tb.blob.Word(word).singularize())\n",
    "        \n",
    "        for index in range(len(sentence_nouns)):\n",
    "            total_count[sentence_nouns[index]] += 1\n",
    "#         for index in range(len(sentence_adjs)):\n",
    "#             total_count[sentence_adjs[index]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### reduce the feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# final feature set\n",
    "reduced_nouns_dic = dict((k, v) for k, v in total_count.items() if (len(k.split(\" \"))==1 and v>=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) count the nouns for each description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 55s, sys: 3.19 s, total: 1min 58s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## -> noun : count\n",
    "def generate_nouns(file_path, indice):\n",
    "    image_nouns = []\n",
    "    for i in indice:\n",
    "        current_count = collections.defaultdict(int)\n",
    "        current_nouns = collections.defaultdict(float)\n",
    "        path = file_path + str(i) + \".txt\"\n",
    "        fo = open(path, \"r\")\n",
    "        for line in fo.readlines():\n",
    "            line = line.strip()\n",
    "            ret = TB(line).lower()\n",
    "            \n",
    "            sentence_nouns=[]\n",
    "            for word,pos in ret.tags:\n",
    "                if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS') and word not in stopwords:\n",
    "                    sentence_nouns.append(tb.blob.Word(word).singularize())\n",
    "\n",
    "            for index in range(len(sentence_nouns)):\n",
    "                current_count[sentence_nouns[index]] += 1\n",
    "                current_nouns[sentence_nouns[index]] = math.log2(current_count[sentence_nouns[index]] + 1)\n",
    "\n",
    "            for word_phrase in ret.noun_phrases:\n",
    "                ret_phrase = TB(word_phrase).lower()\n",
    "                for word,pos in ret_phrase.tags:\n",
    "                    if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "                        temp_noun = tb.blob.Word(word).singularize()\n",
    "                        current_count[temp_noun] += 1\n",
    "                        current_nouns[temp_noun] = math.log2(current_count[temp_noun] + 1)\n",
    "\n",
    "        image_nouns.append(dict(current_nouns))\n",
    "    return image_nouns\n",
    "\n",
    "train_image_nouns = generate_nouns(\"data/descriptions_train/\",train_indice)\n",
    "# validate_image_nouns = generate_nouns(\"data/descriptions_train/\",validate_indice)\n",
    "test_image_nouns = generate_nouns(\"data/descriptions_test/\",test_indice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) prepare inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate training data for each image (feature)\n",
    "feature_name_list = reduced_nouns_dic.keys()\n",
    "def generate_input_features(indice, image_nouns):\n",
    "    image_nouns_features=[]\n",
    "    for i in range(len(indice)):\n",
    "        label=[0 for d in range(len(feature_name_list))]\n",
    "        for index,k in enumerate(feature_name_list):\n",
    "            if k in image_nouns[i]:\n",
    "                label[index]=image_nouns[i][k]\n",
    "        image_nouns_features.append(label)\n",
    "    return image_nouns_features\n",
    "\n",
    "\n",
    "train_image_nouns_features=generate_input_features(train_indice, train_image_nouns)\n",
    "# validate_image_nouns_features=generate_input_features(validate_indice, validate_image_nouns)\n",
    "test_image_nouns_features=generate_input_features(test_indice, test_image_nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "2423\n"
     ]
    }
   ],
   "source": [
    "# train_image_nouns_features\n",
    "print (len(train_image_nouns_features))\n",
    "print (len(train_image_nouns_features[0]))\n",
    "# print (len(test_image_nouns_features))\n",
    "# print (len(test_image_nouns_features[0]))\n",
    "# np.matrix(train_image_nouns_features).shape\n",
    "# np.matrix(train_image_labels['accessory']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdoor\n",
      "sports\n",
      "food\n",
      "appliance\n",
      "furniture\n",
      "indoor\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accessory\n",
      "vehicle\n",
      "kitchen\n",
      "animal\n",
      "electronic\n",
      "CPU times: user 2min 12s, sys: 1.57 s, total: 2min 14s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def train_RF():\n",
    "    regressor_list=[]\n",
    "    for cate in cate_set:\n",
    "        print(cate)\n",
    "        features = np.matrix(train_image_nouns_features)\n",
    "        labels = np.matrix(train_image_labels[cate])\n",
    "\n",
    "        regressor = RandomForestRegressor(n_estimators = 5,max_depth=10,random_state=0)\n",
    "#         regressor = RandomForestRegressor()\n",
    "        regressor.fit(features, labels)\n",
    "        regressor_list.append(regressor)\n",
    "    return regressor_list\n",
    "        \n",
    "regressor_list = train_RF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food\n",
      "animal\n",
      "electronic\n",
      "accessory\n",
      "furniture\n",
      "person\n",
      "vehicle\n",
      "kitchen\n",
      "sports\n",
      "indoor\n",
      "appliance\n",
      "outdoor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_decomposition/pls_.py:77: UserWarning: Maximum number of iterations reached\n",
      "  warnings.warn('Maximum number of iterations reached')\n"
     ]
    }
   ],
   "source": [
    "def train_PLSR():\n",
    "    regressor_list=[]\n",
    "    for cate in cate_set:\n",
    "        print(cate)\n",
    "        features = np.matrix(train_image_nouns_features)\n",
    "        labels = np.matrix(train_image_labels[cate])\n",
    "\n",
    "#         regressor = RandomForestRegressor(n_estimators = 20,min_samples_leaf=10,max_depth=20,random_state=0)\n",
    "        regressor = PLSRegression(n_components=200, scale=True, max_iter=500, tol=1e-06, copy=True)\n",
    "        regressor.fit(features, labels)\n",
    "        regressor_list.append(regressor)\n",
    "    return regressor_list\n",
    "\n",
    "# regressor_list = train_PLSR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food\n",
      "animal\n",
      "electronic\n",
      "accessory\n",
      "furniture\n",
      "person\n",
      "vehicle\n",
      "kitchen\n",
      "sports\n",
      "indoor\n",
      "appliance\n",
      "outdoor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_decomposition/pls_.py:77: UserWarning: Maximum number of iterations reached\n",
      "  warnings.warn('Maximum number of iterations reached')\n"
     ]
    }
   ],
   "source": [
    "def train_MOR():\n",
    "    regressor_list=[]\n",
    "    for cate in cate_set:\n",
    "        print(cate)\n",
    "        features = np.matrix(train_image_nouns_features)\n",
    "        labels = np.matrix(train_image_labels[cate])\n",
    "\n",
    "#         regressor = RandomForestRegressor(n_estimators = 20,min_samples_leaf=10,max_depth=20,random_state=0)\n",
    "#         regressor = PLSRegression(n_components=200, scale=True, max_iter=500, tol=1e-06, copy=True)\n",
    "        regressor = MultiOutputRegressor(estimator, n_jobs=None);\n",
    "        regressor.fit(features, labels)\n",
    "        regressor_list.append(regressor)\n",
    "    return regressor_list\n",
    "\n",
    "# regressor_list = train_PLSR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_labels(image_labels):\n",
    "    final_labels=None\n",
    "    for index,cate in enumerate(cate_set):\n",
    "        temp_matrix = np.matrix(image_labels[cate])\n",
    "        temp_matrix = temp_matrix\n",
    "        if index == 0:\n",
    "            final_labels=temp_matrix\n",
    "        else:\n",
    "            final_labels = np.hstack((final_labels,temp_matrix))\n",
    "    return final_labels\n",
    "\n",
    "train_final_labels=concatenate_labels(train_image_labels)\n",
    "# validate_final_labels=concatenate_labels(validate_image_labels)\n",
    "test_final_labels=concatenate_labels(test_image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 80)\n",
      "(2000, 80)\n"
     ]
    }
   ],
   "source": [
    "print(train_final_labels.shape)\n",
    "# print(validate_final_labels.shape)\n",
    "print(test_final_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_outputs(indice, image_nouns_features, image_labels):\n",
    "    prediction = None\n",
    "    for index,cate in enumerate(cate_set):\n",
    "        features = np.matrix(image_nouns_features)\n",
    "        labels = np.matrix(image_labels[cate])\n",
    "\n",
    "        temp_matrix = regressor_list[index].predict(features).reshape(len(indice),-1)\n",
    "        temp_matrix = temp_matrix\n",
    "        if index == 0:\n",
    "            prediction = temp_matrix\n",
    "        else:\n",
    "            prediction = np.hstack((prediction,temp_matrix))\n",
    "    return prediction\n",
    "            \n",
    "train_predict = concatenate_outputs(train_indice ,train_image_nouns_features, train_image_labels)\n",
    "# validate_predict = concatenate_outputs(validate_indice, validate_image_nouns_features, validate_image_labels)\n",
    "test_predict = concatenate_outputs(test_indice, test_image_nouns_features, test_image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_predict.shape)\n",
    "# print(validate_predict.shape)\n",
    "# print(test_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordering_node(node, nodes):\n",
    "    ordered_indice = np.argsort(distance.cdist(node, nodes))#\n",
    "    return ordered_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_node(node, nodes):\n",
    "    print(int(len(node)/100))\n",
    "    closest_indice = np.argsort(distance.cdist(node, nodes))[:,:int(len(node)/100)]#[:,::-1]\n",
    "    return closest_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_node = train_predict\n",
    "train_nodes = train_final_labels\n",
    "\n",
    "# validate_node = validate_predict\n",
    "# validate_nodes = validate_final_labels\n",
    "\n",
    "test_node = test_predict\n",
    "test_nodes = test_final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_match = closest_node(train_node, train_nodes)\n",
    "# validate_match = closest_node(validate_node, validate_nodes)\n",
    "\n",
    "# print (train_match)\n",
    "# print (validate_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_match_index=[]\n",
    "# train_score = 0\n",
    "# for i in range(len(train_indice)):\n",
    "#     if i in train_match[i]:\n",
    "#         total=(len(train_indice)/100.0)\n",
    "#         train_score += (total-list(train_match[i]).index(i))/total\n",
    "#         train_match_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 20)\n",
      "0.4143250000000001\n"
     ]
    }
   ],
   "source": [
    "# validate_match_index=[]\n",
    "# validate_score = 0\n",
    "\n",
    "# print (validate_match.shape)\n",
    "\n",
    "# for i in range(0, len(validate_indice)):\n",
    "#     if i in validate_match[i]:\n",
    "#         total=(len(validate_indice)/100.0)\n",
    "#         validate_score += (total-list(validate_match[i]).index(i))/total\n",
    "# #         print(list(validate_match[i]).index(i))\n",
    "\n",
    "# print(validate_score/2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1999  724 1633  549 1775 1070 1494  763 1758 1940 1665   91  874  513\n",
      " 1248  384  183 1890 1746 1257]\n",
      "1207\n",
      "0.4143250000000001\n",
      "(2000, 20)\n"
     ]
    }
   ],
   "source": [
    "# print(validate_match[5])\n",
    "# count = 0\n",
    "# score = 0\n",
    "# for i in range(2000):\n",
    "#     if i in validate_match[i]:\n",
    "#         count = count + 1\n",
    "#         score = score + (20  - list(validate_match[i]).index(i)) / 20.0\n",
    "# print(count)\n",
    "# print(score/2000)\n",
    "# print(validate_match.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8009203125000103\n",
      "0.4143250000000001\n"
     ]
    }
   ],
   "source": [
    "# print (train_score/len(train_indice))\n",
    "\n",
    "# print(validate_score/len(validate_indice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 1255,  716, 1904, 1661, 1718, 1663, 1346, 1501, 1898,  974,\n",
       "       1897,  785,  423,  186,  313, 1931, 1750, 1951, 1567])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(validate_match[1270])\n",
    "# validate_match[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1231,   36,  942, 1131,  292,  359, 1743, 1816, 1698, 1862, 1117,\n",
       "        949,  255, 1092,  760, 1258, 1105, 1480,   50,  594])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_match_rank_tag = ordering_node(train_node, train_nodes)\n",
    "test_match = closest_node(test_node, test_nodes)\n",
    "test_match[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(2000):\n",
    "    a = [str(i) + '.txt']\n",
    "    b = ''\n",
    "    for j in range(20):\n",
    "        b += str(test_match[i][j]) + '.jpg '\n",
    "    a.append(b)\n",
    "    result.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('new.csv', result, delimiter = ',', fmt='%s', comments='', header='Descritpion_ID,Top_20_Image_IDs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_match_rank_tag !!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7644, 8885, 4847, ..., 7662, 5553, 9844],\n",
       "       [   1, 5749, 6345, ..., 9138, 4821, 8890],\n",
       "       [6235, 4735, 2181, ..., 2097, 9844, 5553],\n",
       "       ...,\n",
       "       [4787, 2093, 3735, ..., 9844, 8890, 5411],\n",
       "       [9109, 4566, 7301, ..., 3519, 8890, 5411],\n",
       "       [9999, 8290, 3103, ..., 9844, 5411, 5553]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_match_rank_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################  tag  #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################  pool5  #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_pd(label, indice):\n",
    "    new_label=label\n",
    "    for i in range(indice):\n",
    "        img=new_label.iloc[i][0]\n",
    "        # find t, image #\n",
    "        t=''\n",
    "        for j in range(len(img)):\n",
    "            if img[j]=='/':\n",
    "                if img[j+4]=='.':\n",
    "                    t+=img[j+1:j+4]\n",
    "                elif img[j+3]=='.':\n",
    "                    t+=img[j+1:j+3]\n",
    "                elif img[j+5]=='.':\n",
    "                    t+=img[j+1:j+5]\n",
    "                elif img[j+2]=='.':\n",
    "                    t+=img[j+1:j+2]\n",
    "                else:\n",
    "                    t+=img[j+1]\n",
    "                \n",
    "        new_label.set_value(i,0,int(t))\n",
    "    return new_label.sort_values(by=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# sort train_fc1000\n",
    "train_fc1 = pd.DataFrame(pd.read_csv(\"data/features_train/features_resnet1000intermediate_train.csv\", header=-1))\n",
    "train_sorted_fc1=reorder_pd(train_fc1, 10000)\n",
    "train_sorted_fc1000 = train_sorted_fc1.drop(0, 1).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# sort test_fc1000\n",
    "test_fc1 = pd.DataFrame(pd.read_csv(\"data/features_test/features_resnet1000intermediate_test.csv\", header=-1))\n",
    "test_sorted_fc1=reorder_pd(test_fc1, 2000)\n",
    "test_sorted_fc1000 = test_sorted_fc1.drop(0, 1).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training, size of 10000\n",
    "def generate_word_space():\n",
    "    total_count = collections.defaultdict(int)\n",
    "    for i in train_indice:\n",
    "        path = \"data/descriptions_train/\" + str(i) + \".txt\"\n",
    "        fo = open(path, \"r\")\n",
    "        for line in fo.readlines():\n",
    "            line = line.strip()\n",
    "            ret = TB(line).lower()\n",
    "\n",
    "            sentence_words=[]\n",
    "            for word,pos in ret.tags:\n",
    "                if (pos in ['NN','NNP','NNS','NNPS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']) and word not in stopwords:\n",
    "                    sentence_words.append(tb.blob.Word(word).stem())\n",
    "\n",
    "            for index in range(len(sentence_words)):\n",
    "                total_count[sentence_words[index]] += 1\n",
    "    return total_count\n",
    "\n",
    "## -> noun : count\n",
    "def generate_nouns(file_path, indice):\n",
    "    image_nouns = []\n",
    "    for i in indice:\n",
    "#         current_count = collections.defaultdict(int)\n",
    "        current_nouns = collections.defaultdict(float)\n",
    "        path = file_path + str(i) + \".txt\"\n",
    "        fo = open(path, \"r\")\n",
    "        for line in fo.readlines():\n",
    "            line = line.strip()\n",
    "            ret = TB(line).lower()\n",
    "            \n",
    "            sentence_nouns=[]\n",
    "            for word,pos in ret.tags:\n",
    "                if (['NN','NNP','NNS','NNPS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']) and word not in stopwords:\n",
    "                    sentence_nouns.append(tb.blob.Word(word).stem())\n",
    "\n",
    "            for index in range(len(sentence_nouns)):\n",
    "#                 current_count[sentence_nouns[index]] += 1\n",
    "#                 current_nouns[sentence_nouns[index]] = math.log2(current_count[sentence_nouns[index]] + 1)\n",
    "                current_nouns[sentence_nouns[index]] = 1\n",
    "\n",
    "#             for word_phrase in ret.noun_phrases:\n",
    "#                 ret_phrase = TB(word_phrase).lower()\n",
    "#                 for word,pos in ret_phrase.tags:\n",
    "#                     if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS' or pos == 'JJ' or pos == 'JJR' or pos == 'JJS'):\n",
    "#                         temp_noun = tb.blob.Word(word).singularize()\n",
    "#                         current_count[temp_noun] += 1\n",
    "#                         current_nouns[temp_noun] = math.log2(current_count[temp_noun] + 1)\n",
    "\n",
    "        image_nouns.append(dict(current_nouns))\n",
    "    return image_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 15s, sys: 6.87 s, total: 2min 22s\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_image_nouns = generate_nouns(\"data/descriptions_train/\",train_indice)\n",
    "test_image_nouns = generate_nouns(\"data/descriptions_test/\",test_indice)\n",
    "\n",
    "total_count = generate_word_space()\n",
    "reduced_word_space = dict((k, v) for k, v in total_count.items() if v>=3)\n",
    "feature_name_list = reduced_word_space.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3138"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reduced_word_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "def generate_input_features(indice, image_nouns):\n",
    "    image_nouns_features=[]\n",
    "    for i in range(len(indice)):\n",
    "        label=[0 for d in range(len(feature_name_list))]\n",
    "        for index,k in enumerate(feature_name_list):\n",
    "            if k in image_nouns[i]:\n",
    "                label[index]=image_nouns[i][k]\n",
    "        image_nouns_features.append(label)\n",
    "    return image_nouns_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_nouns_features=generate_input_features(train_indice, train_image_nouns)\n",
    "test_image_nouns_features=generate_input_features(test_indice, test_image_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "\n",
    "ridge = Ridge(alpha=115)\n",
    "ridge.fit(train_sorted_fc1000, np.matrix(train_image_nouns_features))\n",
    "train_prediction = ridge.predict(train_sorted_fc1000)\n",
    "test_prediction = ridge.predict(test_sorted_fc1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_node(node, nodes):\n",
    "    print(int(len(node)/100))\n",
    "    closest_indice = np.argsort(distance.cdist(node, nodes))[:,:int(len(node)/100)]#[:,::-1]\n",
    "    return closest_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordering_node_Pool5(node, nodes):\n",
    "    print(int(len(node)/100))\n",
    "    closest_indice = np.argsort(distance.cdist(node, nodes))\n",
    "    return closest_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "100\n",
      "CPU times: user 5min 59s, sys: 4.83 s, total: 6min 4s\n",
      "Wall time: 6min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train_match = closest_node(np.matrix(train_image_nouns_features), train_prediction)\n",
    "\n",
    "test_match = closest_node(np.matrix(test_image_nouns_features), test_prediction)\n",
    "train_match_rank_pool5 = ordering_node_Pool5(np.matrix(train_image_nouns_features), train_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to file\n",
    "\n",
    "result = []\n",
    "for i in range(2000):\n",
    "    a = [str(i) + '.txt']\n",
    "    b = ''\n",
    "    for j in range(20):\n",
    "        b += str(test_match[i][j]) + '.jpg '\n",
    "    a.append(b)\n",
    "    result.append(a)\n",
    "    \n",
    "np.savetxt('new.csv', result, delimiter = ',', fmt='%s', comments='', header='Descritpion_ID,Top_20_Image_IDs')\n",
    "# list(map(lambda x: validate_indice[x],validate_match[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
