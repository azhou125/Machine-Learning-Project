{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/megaandy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## packages\n",
    "import collections\n",
    "import numpy as np\n",
    "import textblob as tb\n",
    "from textblob import TextBlob as TB\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from scipy.spatial import distance\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "import pprint as pp\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressors\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################  description - tag (start)  #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random split to 8000:2000 (train:validate)\n",
    "training_indice = np.random.choice(10000, size=8000, replace=False)\n",
    "validation_indice = [item for item in range(10000) if item not in training_indice]\n",
    "\n",
    "## fix split to 8000:2000 (train:validate)\n",
    "train_indice = range(10000)\n",
    "# validate_indice = range(8000,10000)\n",
    "test_indice = range(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "## translate all tag_file into a dictionary and collect them into an array\n",
    "def tag_reform_collection(source,indice):\n",
    "    tags_records = []\n",
    "    for i in indice:\n",
    "        current_dict = collections.defaultdict(list)\n",
    "        path = source + str(i) +\".txt\"\n",
    "        fo = open(path, \"r\")\n",
    "        for line in fo.readlines():\n",
    "            line = line.strip()\n",
    "            current=line.split(\":\")[0]\n",
    "            current_dict[current].append(line.split(\":\")[1])\n",
    "        tags_records.append(dict(current_dict))\n",
    "    return tags_records\n",
    "        \n",
    "train_tags_records = tag_reform_collection(\"data/tags_train/\",train_indice)\n",
    "# validate_tags_records = tag_reform_collection(\"data/tags_train/\",validate_indice)\n",
    "test_tags_records = tag_reform_collection(\"data/tags_test/\",test_indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cate_set(source,indice):\n",
    "    cate_set = set([])\n",
    "    for i in indice:\n",
    "        path = source + str(i) +\".txt\"\n",
    "        fo = open(path, \"r\")\n",
    "        for line in fo.readlines():\n",
    "            line = line.strip()\n",
    "            cate_set.add(line.split(\":\")[0])\n",
    "    return cate_set\n",
    "\n",
    "train_cate_set = cate_set(\"data/tags_train/\",train_indice)\n",
    "# validate_cate_set = cate_set(\"data/tags_train/\",validate_indice)\n",
    "test_cate_set = cate_set(\"data/tags_test/\",test_indice)\n",
    "cate_set = train_cate_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cate_tags_dictionary(indice, tags_records):\n",
    "    tags_dic = {}\n",
    "    for cate in cate_set:\n",
    "        tags=set()\n",
    "        for i in range(len(indice)):\n",
    "            if cate in tags_records[i]:\n",
    "                tags=tags.union(tags_records[i][cate])\n",
    "        tags=list(tags)\n",
    "        tags.sort()\n",
    "        tags_dic[cate] = tags\n",
    "    return tags_dic\n",
    "\n",
    "train_tags_dic = cate_tags_dictionary(train_indice, train_tags_records)\n",
    "# validate_tags_dic = cate_tags_dictionary(validate_indice, validate_tags_records)\n",
    "test_tags_dic = cate_tags_dictionary(test_indice, test_tags_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train_tags_dic, validate_tags_dic, and test_tags_dic are the same!\n",
    "tags_dic = train_tags_dic\n",
    "# tags_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_labels(indice,tags_records):\n",
    "    image_labels = dict([(key, []) for key in tags_dic])\n",
    "    for i in range(len(indice)):\n",
    "        for k in tags_dic.keys():\n",
    "            label=[0 for d in range(len(tags_dic[k]))]\n",
    "            if k in tags_records[i]:\n",
    "                for index,elt in enumerate(tags_dic[k]):\n",
    "                    if elt in tags_records[i][k]:\n",
    "                        label[index]=1\n",
    "            image_labels[k].append(label)\n",
    "    return image_labels\n",
    "            \n",
    "train_image_labels = build_labels(train_indice,train_tags_records)\n",
    "# validate_image_labels = build_labels(validate_indice,validate_tags_records)\n",
    "test_image_labels = build_labels(test_indice,test_tags_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 s, sys: 737 ms, total: 31.9 s\n",
      "Wall time: 31.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "total_count = collections.defaultdict(int)\n",
    "\n",
    "for i in train_indice:\n",
    "    path = \"data/descriptions_train/\" + str(i) + \".txt\"\n",
    "    fo = open(path, \"r\")\n",
    "    for line in fo.readlines():\n",
    "        line = line.strip()\n",
    "        ret = TB(line).lower()\n",
    "        \n",
    "        sentence_nouns=[]\n",
    "        sentence_adjs=[]\n",
    "        for word,pos in ret.tags:\n",
    "            if (pos == 'NN' or pos == 'NNS') and word not in stopwords:\n",
    "                sentence_nouns.append(tb.blob.Word(word).stem())\n",
    "        \n",
    "        for index in range(len(sentence_nouns)):\n",
    "            total_count[sentence_nouns[index]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final feature set\n",
    "reduced_nouns_dic = dict((k, v) for k, v in total_count.items() if v>=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2982"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reduced_nouns_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.7 s, sys: 834 ms, total: 36.5 s\n",
      "Wall time: 36.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## -> noun : count\n",
    "def generate_nouns(file_path, indice):\n",
    "    image_nouns = []\n",
    "    for i in indice:\n",
    "        current_count = collections.defaultdict(int)\n",
    "        current_nouns = collections.defaultdict(float)\n",
    "        path = file_path + str(i) + \".txt\"\n",
    "        fo = open(path, \"r\")\n",
    "        for line in fo.readlines():\n",
    "            line = line.strip()\n",
    "            ret = TB(line).lower()\n",
    "            \n",
    "            sentence_nouns=[]\n",
    "            for word,pos in ret.tags:\n",
    "                if (pos == 'NN' or pos == 'NNS') and word not in stopwords:\n",
    "                    sentence_nouns.append(tb.blob.Word(word).stem())\n",
    "\n",
    "            for index in range(len(sentence_nouns)):\n",
    "                current_count[sentence_nouns[index]] = 1\n",
    "                current_nouns[sentence_nouns[index]] = 1\n",
    "\n",
    "        image_nouns.append(dict(current_nouns))\n",
    "    return image_nouns\n",
    "\n",
    "train_image_nouns = generate_nouns(\"data/descriptions_train/\",train_indice)\n",
    "# validate_image_nouns = generate_nouns(\"data/descriptions_train/\",validate_indice)\n",
    "test_image_nouns = generate_nouns(\"data/descriptions_test/\",test_indice)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate training data for each image (feature)\n",
    "feature_name_list = reduced_nouns_dic.keys()\n",
    "def generate_input_features(indice, image_nouns):\n",
    "    image_nouns_features=[]\n",
    "    for i in range(len(indice)):\n",
    "        label=[0 for d in range(len(feature_name_list))]\n",
    "        for index,k in enumerate(feature_name_list):\n",
    "            if k in image_nouns[i]:\n",
    "                label[index]=image_nouns[i][k]\n",
    "        image_nouns_features.append(label)\n",
    "    return image_nouns_features\n",
    "\n",
    "\n",
    "train_image_nouns_features=generate_input_features(train_indice, train_image_nouns)\n",
    "# validate_image_nouns_features=generate_input_features(validate_indice, validate_image_nouns)\n",
    "test_image_nouns_features=generate_input_features(test_indice, test_image_nouns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2982)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matrix(train_image_nouns_features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vehicle\n",
      "furniture\n",
      "outdoor\n",
      "indoor\n",
      "food\n",
      "accessory\n",
      "kitchen\n",
      "electronic\n",
      "sports\n",
      "appliance\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal\n",
      "CPU times: user 55min 28s, sys: 3.39 s, total: 55min 32s\n",
      "Wall time: 55min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def train_RF():\n",
    "    regressor_list=[]\n",
    "    for cate in cate_set:\n",
    "        print(cate)\n",
    "        features = np.matrix(train_image_nouns_features)\n",
    "        labels = np.matrix(train_image_labels[cate])\n",
    "\n",
    "        regressor = RandomForestRegressor(n_estimators = 10)\n",
    "\n",
    "        regressor.fit(features, labels)\n",
    "        regressor_list.append(regressor)\n",
    "    return regressor_list\n",
    "        \n",
    "regressor_list = train_RF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_list_Tag=regressor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_labels(image_labels):\n",
    "    final_labels=None\n",
    "    for index,cate in enumerate(cate_set):\n",
    "        temp_matrix = np.matrix(image_labels[cate])\n",
    "        temp_matrix = temp_matrix\n",
    "        if index == 0:\n",
    "            final_labels=temp_matrix\n",
    "        else:\n",
    "            final_labels = np.hstack((final_labels,temp_matrix))\n",
    "    return final_labels\n",
    "\n",
    "train_final_labels=concatenate_labels(train_image_labels)\n",
    "# validate_final_labels=concatenate_labels(validate_image_labels)\n",
    "test_final_labels=concatenate_labels(test_image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_outputs(indice, image_nouns_features, image_labels):\n",
    "    prediction = None\n",
    "    for index,cate in enumerate(cate_set):\n",
    "        features = np.matrix(image_nouns_features)\n",
    "        labels = np.matrix(image_labels[cate])\n",
    "\n",
    "        temp_matrix = regressor_list[index].predict(features).reshape(len(indice),-1)\n",
    "        temp_matrix = temp_matrix\n",
    "        if index == 0:\n",
    "            prediction = temp_matrix\n",
    "        else:\n",
    "            prediction = np.hstack((prediction,temp_matrix))\n",
    "    return prediction\n",
    "            \n",
    "train_predict = concatenate_outputs(train_indice ,train_image_nouns_features, train_image_labels)\n",
    "# validate_predict = concatenate_outputs(validate_indice, validate_image_nouns_features, validate_image_labels)\n",
    "test_predict = concatenate_outputs(test_indice, test_image_nouns_features, test_image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def closest_node(node, nodes):\n",
    "#     print(int(len(node)/100))\n",
    "#     closest_indice = np.argsort(distance.cdist(node, nodes))[:,:int(len(node)/100)]\n",
    "#     return closest_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ordering_node(node, nodes):\n",
    "#     ordered_indice = np.argsort(distance.cdist(node, nodes))#\n",
    "#     return ordered_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_node_cosine(node, nodes):\n",
    "    closest_indice = np.argsort(-cosine_similarity(node, nodes))[:,:int(len(node)/100)]\n",
    "    return closest_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordering_node_cosine(node, nodes):\n",
    "    ordered_indice = np.argsort(-cosine_similarity(node, nodes))\n",
    "    return ordered_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_node = train_predict\n",
    "train_nodes = train_final_labels\n",
    "\n",
    "# validate_node = validate_predict\n",
    "# validate_nodes = validate_final_labels\n",
    "\n",
    "test_node = test_predict\n",
    "test_nodes = test_final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_match_cosine = ordering_node_cosine(train_node, train_nodes)\n",
    "test_match_cosine = closest_node_cosine(test_node, test_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(2000):\n",
    "    a = [str(i) + '.txt']\n",
    "    b = ''\n",
    "    for j in range(20):\n",
    "        b += str(test_match_cosine[i,j]) + '.jpg '\n",
    "    a.append(b)\n",
    "    result.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('new.csv', result, delimiter = ',', fmt='%s', comments='', header='Descritpion_ID,Top_20_Image_IDs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################  description - tag (end)  #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################  pool5 - description (start) #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_pd(label, indice):\n",
    "    new_label=label\n",
    "    for i in range(indice):\n",
    "        img=new_label.iloc[i][0]\n",
    "        # find t, image #\n",
    "        t=''\n",
    "        for j in range(len(img)):\n",
    "            if img[j]=='/':\n",
    "                if img[j+4]=='.':\n",
    "                    t+=img[j+1:j+4]\n",
    "                elif img[j+3]=='.':\n",
    "                    t+=img[j+1:j+3]\n",
    "                elif img[j+5]=='.':\n",
    "                    t+=img[j+1:j+5]\n",
    "                elif img[j+2]=='.':\n",
    "                    t+=img[j+1:j+2]\n",
    "                else:\n",
    "                    t+=img[j+1]\n",
    "                \n",
    "        new_label.set_value(i,0,int(t))\n",
    "    return new_label.sort_values(by=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# sort train_fc1000\n",
    "train_fc1 = pd.DataFrame(pd.read_csv(\"data/features_train/features_resnet1000intermediate_train.csv\", header=-1))\n",
    "train_sorted_fc1=reorder_pd(train_fc1, 10000)\n",
    "train_sorted_fc1000 = train_sorted_fc1.drop(0, 1).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# sort test_fc1000\n",
    "test_fc1 = pd.DataFrame(pd.read_csv(\"data/features_test/features_resnet1000intermediate_test.csv\", header=-1))\n",
    "test_sorted_fc1=reorder_pd(test_fc1, 2000)\n",
    "test_sorted_fc1000 = test_sorted_fc1.drop(0, 1).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training, size of 10000\n",
    "def generate_word_space():\n",
    "    total_count = collections.defaultdict(int)\n",
    "    for i in train_indice:\n",
    "        path = \"data/descriptions_train/\" + str(i) + \".txt\"\n",
    "        fo = open(path, \"r\")\n",
    "        for line in fo.readlines():\n",
    "            line = line.strip()\n",
    "            ret = TB(line).lower()\n",
    "\n",
    "            sentence_words=[]\n",
    "            for word,pos in ret.tags:\n",
    "                if (pos in ['NN','NNS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']) and word not in stopwords:\n",
    "                    sentence_words.append(tb.blob.Word(word).stem())\n",
    "\n",
    "            for index in range(len(sentence_words)):\n",
    "                total_count[sentence_words[index]] += 1\n",
    "    return total_count\n",
    "\n",
    "## -> noun : count\n",
    "def generate_nouns(file_path, indice):\n",
    "    image_nouns = []\n",
    "    for i in indice:\n",
    "#         current_count = collections.defaultdict(int)\n",
    "        current_nouns = collections.defaultdict(float)\n",
    "        path = file_path + str(i) + \".txt\"\n",
    "        fo = open(path, \"r\")\n",
    "        for line in fo.readlines():\n",
    "            line = line.strip()\n",
    "            ret = TB(line).lower()\n",
    "            \n",
    "            sentence_nouns=[]\n",
    "            for word,pos in ret.tags:\n",
    "                if (['NN','NNS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']) and word not in stopwords:\n",
    "                    sentence_nouns.append(tb.blob.Word(word).lemmatize())\n",
    "\n",
    "            for index in range(len(sentence_nouns)):\n",
    "                current_nouns[sentence_nouns[index]] = 1\n",
    "\n",
    "        image_nouns.append(dict(current_nouns))\n",
    "    return image_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 1.85 s, total: 1min 14s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_image_nouns = generate_nouns(\"data/descriptions_train/\",train_indice)\n",
    "test_image_nouns = generate_nouns(\"data/descriptions_test/\",test_indice)\n",
    "\n",
    "total_count = generate_word_space()\n",
    "reduced_word_space = dict((k, v) for k, v in total_count.items() if v>=3)\n",
    "feature_name_list = reduced_word_space.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3138"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reduced_word_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "def generate_input_features(indice, image_nouns):\n",
    "    image_nouns_features=[]\n",
    "    for i in range(len(indice)):\n",
    "        label=[0 for d in range(len(feature_name_list))]\n",
    "        for index,k in enumerate(feature_name_list):\n",
    "            if k in image_nouns[i]:\n",
    "                label[index]=image_nouns[i][k]\n",
    "        image_nouns_features.append(label)\n",
    "    return image_nouns_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_nouns_features=generate_input_features(train_indice, train_image_nouns)\n",
    "test_image_nouns_features=generate_input_features(test_indice, test_image_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "\n",
    "ridge = Ridge(alpha=115)\n",
    "ridge.fit(train_sorted_fc1000, np.matrix(train_image_nouns_features))\n",
    "train_prediction = ridge.predict(train_sorted_fc1000)\n",
    "test_prediction = ridge.predict(test_sorted_fc1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_node(node, nodes):\n",
    "    print(int(len(node)/100))\n",
    "    closest_indice = np.argsort(distance.cdist(node, nodes))[:,:int(len(node)/100)]#[:,::-1]\n",
    "    return closest_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordering_node_Pool5(node, nodes):\n",
    "    print(int(len(node)/100))\n",
    "    closest_indice = np.argsort(distance.cdist(node, nodes))\n",
    "    return closest_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "100\n",
      "CPU times: user 5min 36s, sys: 1.39 s, total: 5min 37s\n",
      "Wall time: 5min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train_match = closest_node(np.matrix(train_image_nouns_features), train_prediction)\n",
    "\n",
    "test_match_pool5 = closest_node(np.matrix(test_image_nouns_features), test_prediction)\n",
    "train_match_rank_pool5 = ordering_node_Pool5(np.matrix(train_image_nouns_features), train_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(2000):\n",
    "    a = [str(i) + '.txt']\n",
    "    b = ''\n",
    "    for j in range(20):\n",
    "        b += str(test_match_pool5[i,j]) + '.jpg '\n",
    "    a.append(b)\n",
    "    result.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('new.csv', result, delimiter = ',', fmt='%s', comments='', header='Descritpion_ID,Top_20_Image_IDs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################  pool5 - description (end) #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## further try for combing two models (not finished) ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0 for d in range(10000)]\n",
    "for i in range(10000):\n",
    "    index1 = list(train_match_rank_tag[i]).index(i)\n",
    "    index2 = list(train_match_rank_pool5[i]).index(i)\n",
    "    if index1<index2 :\n",
    "        labels[i] = 0\n",
    "    else:\n",
    "        labels[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in labels if i==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_nouns_features=np.matrix(train_image_nouns_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(train_image_nouns_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_picking_prediction = logisticRegr.predict(test_image_nouns_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in model_picking_prediction if i==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2000):\n",
    "    test_image_nouns_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
