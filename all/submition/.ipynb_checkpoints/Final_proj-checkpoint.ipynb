{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/guosihong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import textblob as tb\n",
    "from textblob import TextBlob as TB\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the images_tags for easy accessibility\n",
    "# Image_tags: 10000 dicts for 10000 images\n",
    "train_image_tags = []\n",
    "test_image_tags = []\n",
    "for i in range(10000):\n",
    "    current_dict = collections.defaultdict(list)\n",
    "    path = \"/Users/guosihong/Desktop/AML/final/all/data/tags_train/\" + str(i) +\".txt\"\n",
    "    fo = open(path, \"r\")\n",
    "    for line in fo.readlines():\n",
    "        line = line.strip()\n",
    "        current=line.split(\":\")[0]\n",
    "        current_dict[current].append(line.split(\":\")[1])\n",
    "    train_image_tags.append(dict(current_dict))\n",
    "for i in range(2000):\n",
    "    current_dict = collections.defaultdict(list)\n",
    "    path = \"/Users/guosihong/Desktop/AML/final/all/data/tags_test/\" + str(i) +\".txt\"\n",
    "    fo = open(path, \"r\")\n",
    "    for line in fo.readlines():\n",
    "        line = line.strip()\n",
    "        current=line.split(\":\")[0]\n",
    "        current_dict[current].append(line.split(\":\")[1])\n",
    "    test_image_tags.append(dict(current_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a set of category\n",
    "train_cate_set= set([])\n",
    "test_cate_set= set([])\n",
    "for i in range(0, 10000):\n",
    "    path = \"/Users/guosihong/Desktop/AML/final/all/data/tags_train/\" + str(i) +\".txt\"\n",
    "    fo = open(path, \"r\")\n",
    "    for line in fo.readlines():\n",
    "        line = line.strip()\n",
    "        train_cate_set.add(line.split(\":\")[0])\n",
    "for i in range(0, 2000):\n",
    "    path = \"/Users/guosihong/Desktop/AML/final/all/data/tags_test/\" + str(i) +\".txt\"\n",
    "    fo = open(path, \"r\")\n",
    "    for line in fo.readlines():\n",
    "        line = line.strip()\n",
    "        test_cate_set.add(line.split(\":\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_cate_set and test_cate_set are the same!\n",
    "cate_set = train_cate_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect tags set for each category:\n",
    "train_tags_dic={}\n",
    "test_tags_dic={}\n",
    "for cate in cate_set:\n",
    "    tags=set()\n",
    "    for i in range(10000):\n",
    "        if cate in train_image_tags[i]:\n",
    "            tags=tags.union(train_image_tags[i][cate])\n",
    "    tags=list(tags)\n",
    "    tags.sort()\n",
    "    train_tags_dic[cate] = tags\n",
    "for cate in cate_set:\n",
    "    tags=set()\n",
    "    for i in range(2000):\n",
    "        if cate in test_image_tags[i]:\n",
    "            tags=tags.union(test_image_tags[i][cate])\n",
    "    tags=list(tags)\n",
    "    tags.sort()\n",
    "    test_tags_dic[cate] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tags_dic and test_tags_dic are the same!\n",
    "tags_dic = train_tags_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_labels=dict([(key, []) for key in tags_dic])\n",
    "test_image_labels=dict([(key, []) for key in tags_dic])\n",
    "for i in range(10000):\n",
    "    for k in tags_dic.keys():\n",
    "        label=[0 for d in range(len(tags_dic[k]))]\n",
    "        if k in train_image_tags[i]:\n",
    "            for index,elt in enumerate(tags_dic[k]):\n",
    "                if elt in train_image_tags[i][k]:\n",
    "                    label[index]=1\n",
    "        train_image_labels[k].append(label)\n",
    "for i in range(2000):\n",
    "    for k in tags_dic.keys():\n",
    "        label=[0 for d in range(len(tags_dic[k]))]\n",
    "        if k in test_image_tags[i]:\n",
    "            for index,elt in enumerate(tags_dic[k]):\n",
    "                if elt in test_image_tags[i][k]:\n",
    "                    label[index]=1\n",
    "        test_image_labels[k].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 6.41 s, total: 1min 31s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "total_count = collections.defaultdict(int)\n",
    "total_nouns = collections.defaultdict(float)\n",
    "image_nouns = []\n",
    "for i in range(10000):\n",
    "    current_count = collections.defaultdict(int)\n",
    "    current_nouns = collections.defaultdict(float)\n",
    "    path = \"/Users/guosihong/Desktop/AML/final/all/data/descriptions_train/\" + str(i) + \".txt\"\n",
    "    fo = open(path, \"r\")\n",
    "    for line in fo.readlines():\n",
    "        line = line.strip()\n",
    "        ret = TB(line).lower()\n",
    "        \n",
    "        sentence_nouns=[]\n",
    "        for word,pos in ret.tags:\n",
    "            if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS') and word not in stopwords:\n",
    "                sentence_nouns.append(tb.blob.Word(word).singularize())\n",
    "        \n",
    "        for index in range(len(sentence_nouns)):\n",
    "            total_count[sentence_nouns[index]] += 1\n",
    "            current_count[sentence_nouns[index]] += 1\n",
    "            current_nouns[sentence_nouns[index]] = math.log2(current_count[sentence_nouns[index]] + 1)\n",
    "            \n",
    "        for word_phrase in ret.noun_phrases:\n",
    "            ret_phrase = TB(word_phrase).lower()\n",
    "            for word,pos in ret_phrase.tags:\n",
    "                if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "                    temp_noun = tb.blob.Word(word).singularize()\n",
    "                    current_count[word] += 1\n",
    "                    current_nouns[word] = math.log2(current_count[word] + 1)\n",
    "            \n",
    "    image_nouns.append(dict(current_nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_nouns = []\n",
    "for i in range(2000):\n",
    "    current_count = collections.defaultdict(int)\n",
    "    current_nouns = collections.defaultdict(float)\n",
    "    path = \"/Users/guosihong/Desktop/AML/final/all/data/descriptions_test/\" + str(i) + \".txt\"\n",
    "    fo = open(path, \"r\")\n",
    "    for line in fo.readlines():\n",
    "        line = line.strip()\n",
    "        ret = TB(line).lower()\n",
    "        \n",
    "        sentence_nouns=[]\n",
    "        for word,pos in ret.tags:\n",
    "            if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS') and word not in stopwords:\n",
    "                sentence_nouns.append(tb.blob.Word(word).singularize())\n",
    "        \n",
    "        for index in range(len(sentence_nouns)):\n",
    "            current_count[sentence_nouns[index]] += 1\n",
    "            current_nouns[sentence_nouns[index]] = math.log2(current_count[sentence_nouns[index]] + 1)\n",
    "            \n",
    "        for word_phrase in ret.noun_phrases:\n",
    "            ret_phrase = TB(word_phrase).lower()\n",
    "            for word,pos in ret_phrase.tags:\n",
    "                if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "                    temp_noun = tb.blob.Word(word).singularize()\n",
    "                    current_count[word] += 1\n",
    "                    current_nouns[word] = math.log2(current_count[word] + 1)\n",
    "    test_image_nouns.append(dict(current_nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduced_nouns_dic = dict((k, v) for k, v in total_count.items() if (len(k.split(\" \"))==1 and v>=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.17 s, sys: 100 ms, total: 4.27 s\n",
      "Wall time: 4.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_image_nouns_features=[]\n",
    "test_image_nouns_features=[]\n",
    "feature_name_list = reduced_nouns_dic.keys()\n",
    "for i in range(10000):\n",
    "    label=[0 for d in range(len(feature_name_list))]\n",
    "    for index,k in enumerate(feature_name_list):\n",
    "        if k in image_nouns[i]:\n",
    "            label[index]=image_nouns[i][k]\n",
    "    train_image_nouns_features.append(label)\n",
    "for i in range(2000):\n",
    "    label=[0 for d in range(len(feature_name_list))]\n",
    "    for index,k in enumerate(feature_name_list):\n",
    "        if k in test_image_nouns[i]:\n",
    "            label[index]=test_image_nouns[i][k]\n",
    "    test_image_nouns_features.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sports\n",
      "animal\n",
      "accessory\n",
      "outdoor\n",
      "appliance\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electronic\n",
      "vehicle\n",
      "furniture\n",
      "kitchen\n",
      "indoor\n",
      "food\n",
      "CPU times: user 4min 37s, sys: 1.93 s, total: 4min 39s\n",
      "Wall time: 4min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regressor_list=[]\n",
    "for cate in cate_set:\n",
    "    print(cate)\n",
    "    features = np.matrix(train_image_nouns_features)\n",
    "    labels = np.matrix(train_image_labels[cate])\n",
    "    regressor = RandomForestRegressor(n_estimators = 10,min_samples_leaf=20,max_depth=20,random_state=0)\n",
    "    regressor.fit(features, labels)\n",
    "    regressor_list.append(regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=20, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=20, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=20, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=20, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=20, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=20, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=20, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=20, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=20, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=20, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=20, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=20, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_labels=None\n",
    "for index,cate in enumerate(cate_set):\n",
    "    if index == 0:\n",
    "        train_final_labels=np.matrix(train_image_labels[cate])\n",
    "    else:\n",
    "        train_final_labels = np.hstack((train_final_labels,np.matrix(train_image_labels[cate])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 80)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_labels=None\n",
    "for index,cate in enumerate(cate_set):\n",
    "    if index == 0:\n",
    "        test_final_labels=np.matrix(test_image_labels[cate])\n",
    "    else:\n",
    "        test_final_labels = np.hstack((test_final_labels,np.matrix(test_image_labels[cate])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 80)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict=None\n",
    "for index,cate in enumerate(cate_set):\n",
    "    features = np.matrix(train_image_nouns_features)\n",
    "    labels = np.matrix(train_image_labels[cate])\n",
    "    if index == 0:\n",
    "        train_predict = regressor_list[index].predict(features).reshape(10000,-1)\n",
    "    else:\n",
    "        train_predict = np.hstack((train_predict,regressor_list[index].predict(features).reshape(10000,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 80)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict=None\n",
    "for index,cate in enumerate(cate_set):\n",
    "    features = np.matrix(test_image_nouns_features)\n",
    "    labels = np.matrix(test_image_labels[cate])\n",
    "    if index == 0:\n",
    "        test_predict = regressor_list[index].predict(features).reshape(2000,-1)\n",
    "    else:\n",
    "        test_predict = np.hstack((test_predict,regressor_list[index].predict(features).reshape(2000,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 80)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_node(node, nodes):\n",
    "    closest_indice = np.argsort(distance.cdist(node, nodes))[:,:20]\n",
    "    return closest_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_node = train_predict\n",
    "train_nodes = train_final_labels\n",
    "test_node = test_predict\n",
    "test_nodes = test_final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_match = closest_node(train_node, train_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[]\n",
    "score = 0\n",
    "for i in range(10000):\n",
    "    if i in train_match[i]:\n",
    "        score += (20-list(train_match[i]).index(i))/20.0\n",
    "        A.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2511"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16250000000000023"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_match = closest_node(test_node, test_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 292,  942,  359, ..., 1092,  760,  594],\n",
       "       [1156, 1529, 1341, ..., 1027,  607, 1574],\n",
       "       [1724,  184,  445, ..., 1366, 1471,  838],\n",
       "       ...,\n",
       "       [ 104, 1026,  657, ...,  357, 1726, 1312],\n",
       "       [1135, 1429,  897, ...,  436, 1139, 1072],\n",
       "       [ 729,  653, 1342, ..., 1171,  976,  468]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 20)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_match.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 292  942  359 1743   36 1131 1231 1698 1862  124 1816 1105 1480  255\n",
      " 1258 1117  949 1092  760  594]\n"
     ]
    }
   ],
   "source": [
    "print(test_match[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(2000):\n",
    "    a = [str(i) + '.txt']\n",
    "    b = ''\n",
    "    for j in range(20):\n",
    "        b += str(test_match[i][j]) + '.jpg '\n",
    "    a.append(b)\n",
    "    result.append(a)\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([['0.txt',\n",
       "         '292.jpg 942.jpg 359.jpg 1743.jpg 36.jpg 1131.jpg 1231.jpg 1698.jpg 1862.jpg 124.jpg 1816.jpg 1105.jpg 1480.jpg 255.jpg 1258.jpg 1117.jpg 949.jpg 1092.jpg 760.jpg 594.jpg '],\n",
       "        ['1.txt',\n",
       "         '1156.jpg 1529.jpg 1341.jpg 833.jpg 589.jpg 758.jpg 1907.jpg 1929.jpg 753.jpg 435.jpg 948.jpg 1033.jpg 1030.jpg 1421.jpg 1059.jpg 1648.jpg 763.jpg 1027.jpg 607.jpg 1574.jpg '],\n",
       "        ['2.txt',\n",
       "         '1724.jpg 184.jpg 445.jpg 1665.jpg 649.jpg 1866.jpg 1124.jpg 1840.jpg 159.jpg 1713.jpg 1107.jpg 953.jpg 1955.jpg 720.jpg 1242.jpg 3.jpg 598.jpg 1366.jpg 1471.jpg 838.jpg '],\n",
       "        ...,\n",
       "        ['1997.txt',\n",
       "         '104.jpg 1026.jpg 657.jpg 1481.jpg 293.jpg 1660.jpg 1429.jpg 1218.jpg 1135.jpg 152.jpg 141.jpg 1535.jpg 897.jpg 1638.jpg 58.jpg 1192.jpg 199.jpg 357.jpg 1726.jpg 1312.jpg '],\n",
       "        ['1998.txt',\n",
       "         '1135.jpg 1429.jpg 897.jpg 1660.jpg 152.jpg 1218.jpg 141.jpg 1535.jpg 1388.jpg 861.jpg 895.jpg 1481.jpg 597.jpg 911.jpg 621.jpg 318.jpg 793.jpg 436.jpg 1139.jpg 1072.jpg '],\n",
       "        ['1999.txt',\n",
       "         '729.jpg 653.jpg 1342.jpg 1430.jpg 926.jpg 1200.jpg 1135.jpg 1218.jpg 1660.jpg 1535.jpg 141.jpg 1429.jpg 897.jpg 152.jpg 1852.jpg 1409.jpg 1589.jpg 1171.jpg 976.jpg 468.jpg ']],\n",
       "       dtype='<U177')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matrix(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('new.csv', result, delimiter = ',', fmt='%s', header='Descritpion_ID')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
