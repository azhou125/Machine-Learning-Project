{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Shawn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## packages\n",
    "import collections\n",
    "import numpy as np\n",
    "import textblob as tb\n",
    "from textblob import TextBlob as TB\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from scipy.spatial import distance\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "import pprint as pp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressors\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.linear_model import Lasso\n",
    "# from sklearn.linear_model import ElasticNet\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random split to 8000:2000 (train:validate)\n",
    "# train_indice = np.random.choice(10000, size=10000, replace=False)\n",
    "# validate_indice = [item for item in range(10000) if item not in train_indice]\n",
    "\n",
    "## fix split to 8000:2000 (train:validate)\n",
    "train_indice = range(10000)\n",
    "# validate_indice = range(8000,10000)\n",
    "test_indice = range(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_pd(label, indice):\n",
    "    new_label=label\n",
    "    for i in range(indice):\n",
    "        img=new_label.iloc[i][0]\n",
    "        # find t, image #\n",
    "        t=''\n",
    "        for j in range(len(img)):\n",
    "            if img[j]=='/':\n",
    "                if img[j+4]=='.':\n",
    "                    t+=img[j+1:j+4]\n",
    "                elif img[j+3]=='.':\n",
    "                    t+=img[j+1:j+3]\n",
    "                elif img[j+5]=='.':\n",
    "                    t+=img[j+1:j+5]\n",
    "                elif img[j+2]=='.':\n",
    "                    t+=img[j+1:j+2]\n",
    "                else:\n",
    "                    t+=img[j+1]\n",
    "                \n",
    "        new_label.set_value(i,0,int(t))\n",
    "    return new_label.sort_values(by=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# sort train_fc1000\n",
    "train_fc1 = pd.DataFrame(pd.read_csv(\"data/features_train/features_resnet1000_train.csv\", header=-1))\n",
    "train_sorted_fc1=reorder_pd(train_fc1, 10000)\n",
    "train_sorted_fc1000 = train_sorted_fc1.drop(0, 1).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# sort test_fc1000\n",
    "test_fc1 = pd.DataFrame(pd.read_csv(\"data/features_test/features_resnet1000_test.csv\", header=-1))\n",
    "test_sorted_fc1=reorder_pd(test_fc1, 2000)\n",
    "test_sorted_fc1000 = test_sorted_fc1.drop(0, 1).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training, size of 10000\n",
    "def generate_word_space():\n",
    "    total_count = collections.defaultdict(int)\n",
    "    for i in train_indice:\n",
    "        path = \"data/descriptions_train/\" + str(i) + \".txt\"\n",
    "        fo = open(path, \"r\")\n",
    "        for line in fo.readlines():\n",
    "            line = line.strip()\n",
    "            ret = TB(line).lower()\n",
    "\n",
    "            sentence_words=[]\n",
    "            for word,pos in ret.tags:\n",
    "                if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS' or pos == 'JJ' or pos == 'JJR' or pos == 'JJS') and word not in stopwords:\n",
    "                    sentence_words.append(tb.blob.Word(word).stem())\n",
    "\n",
    "            for index in range(len(sentence_words)):\n",
    "                total_count[sentence_words[index]] += 1\n",
    "    return total_count\n",
    "\n",
    "## -> noun : count\n",
    "def generate_nouns(file_path, indice):\n",
    "    image_nouns = []\n",
    "    for i in indice:\n",
    "#         current_count = collections.defaultdict(int)\n",
    "        current_nouns = collections.defaultdict(float)\n",
    "        path = file_path + str(i) + \".txt\"\n",
    "        fo = open(path, \"r\")\n",
    "        for line in fo.readlines():\n",
    "            line = line.strip()\n",
    "            ret = TB(line).lower()\n",
    "            \n",
    "            sentence_nouns=[]\n",
    "            for word,pos in ret.tags:\n",
    "                if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS' or pos == 'JJ' or pos == 'JJR' or pos == 'JJS') and word not in stopwords:\n",
    "                    sentence_nouns.append(tb.blob.Word(word).stem())\n",
    "\n",
    "            for index in range(len(sentence_nouns)):\n",
    "#                 current_count[sentence_nouns[index]] += 1\n",
    "#                 current_nouns[sentence_nouns[index]] = math.log2(current_count[sentence_nouns[index]] + 1)\n",
    "                current_nouns[sentence_nouns[index]] = 1\n",
    "\n",
    "#             for word_phrase in ret.noun_phrases:\n",
    "#                 ret_phrase = TB(word_phrase).lower()\n",
    "#                 for word,pos in ret_phrase.tags:\n",
    "#                     if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS' or pos == 'JJ' or pos == 'JJR' or pos == 'JJS'):\n",
    "#                         temp_noun = tb.blob.Word(word).singularize()\n",
    "#                         current_count[temp_noun] += 1\n",
    "#                         current_nouns[temp_noun] = math.log2(current_count[temp_noun] + 1)\n",
    "\n",
    "        image_nouns.append(dict(current_nouns))\n",
    "    return image_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_nouns = generate_nouns(\"data/descriptions_train/\",train_indice)\n",
    "test_image_nouns = generate_nouns(\"data/descriptions_test/\",test_indice)\n",
    "\n",
    "total_count = generate_word_space()\n",
    "reduced_word_space = dict((k, v) for k, v in total_count.items() if v>=2)\n",
    "feature_name_list = reduced_word_space.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3642"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reduced_word_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "def generate_input_features(indice, image_nouns):\n",
    "    image_nouns_features=[]\n",
    "    for i in range(len(indice)):\n",
    "        label=[0 for d in range(len(feature_name_list))]\n",
    "        for index,k in enumerate(feature_name_list):\n",
    "            if k in image_nouns[i]:\n",
    "                label[index]=image_nouns[i][k]\n",
    "        image_nouns_features.append(label)\n",
    "    return image_nouns_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_nouns_features=generate_input_features(train_indice, train_image_nouns)\n",
    "test_image_nouns_features=generate_input_features(test_indice, test_image_nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp here, for train and predict\n",
    "#                         train_sorted_fc1000\n",
    "#                         train_image_nouns_features\n",
    "\n",
    "#                         test_sorted_fc1000\n",
    "#                         test_image_nouns_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# np.matrix(train_image_nouns_features)\n",
    "print(type(train_sorted_fc1000))\n",
    "# print (np.matrix(train_image_nouns_features).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "\n",
    "# ridge = Ridge()\n",
    "ridge = Ridge(alpha=450, tol = 0.001)\n",
    "ridge.fit(train_sorted_fc1000, np.matrix(train_image_nouns_features))\n",
    "train_prediction = ridge.predict(train_sorted_fc1000)\n",
    "test_prediction = ridge.predict(test_sorted_fc1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_node(node, nodes):\n",
    "    print(int(len(node)/100))\n",
    "    closest_indice = np.argsort(distance.cdist(node, nodes))[:,:int(len(node)/100)]#[:,::-1]\n",
    "    return closest_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "3642\n",
      "10000\n",
      "3642\n"
     ]
    }
   ],
   "source": [
    "print(len(train_image_nouns_features))\n",
    "print(len(train_image_nouns_features[0]))\n",
    "\n",
    "\n",
    "print(len(train_prediction))\n",
    "print(len(train_prediction[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.matrixlib.defmatrix.matrix'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(np.matrix(train_image_nouns_features)))\n",
    "print(type(train_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training score\n",
    "# train_match = closest_node(np.matrix(train_image_nouns_features), train_prediction)\n",
    "# train_match.shape\n",
    "\n",
    "# train_match_index=[]\n",
    "# train_score = 0\n",
    "# for i in range(len(train_indice)):\n",
    "#     if i in train_match[i]:\n",
    "#         total=(len(train_indice)/100.0)\n",
    "#         train_score += (total-list(train_match[i]).index(i))/total\n",
    "#         train_match_index.append(i)\n",
    "# train_score/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "CPU times: user 16.4 s, sys: 91 ms, total: 16.4 s\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_match = closest_node(np.matrix(test_image_nouns_features), test_prediction)\n",
    "\n",
    "# output to file\n",
    "result = []\n",
    "for i in range(2000):\n",
    "    a = [str(i) + '.txt']\n",
    "    b = ''\n",
    "    for j in range(20):\n",
    "        b += str(test_match[i][j]) + '.jpg '\n",
    "    a.append(b)\n",
    "    result.append(a)\n",
    "    \n",
    "np.savetxt('new.csv', result, delimiter = ',', fmt='%s', comments='', header='Descritpion_ID,Top_20_Image_IDs')\n",
    "# list(map(lambda x: validate_indice[x],validate_match[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 100 -> 0.38476\n",
    "\n",
    "# 50  -> 0.38674\n",
    "\n",
    "# 47  -> 0.38754\n",
    "# 45  -> 0.38830\n",
    "# 40  -> 0.38758\n",
    "\n",
    "# 16  -> 0.3861\n",
    "# 15  -> 0.38448   0.001\n",
    "# 15  -> 0.38448   0.0001\n",
    "    # 14  -> 0.38494\n",
    "# 13  -> 0.38388\n",
    "# 12  -> 0.38403\n",
    "# 11  -> 0.38299\n",
    "# 10  -> 0.38186\n",
    "# 9.0 -> 0.38164\n",
    "# 8.0 -> 0.37962\n",
    "# 7.0 -> 0.37914\n",
    "# 6.0 -> 0.37646\n",
    "# 5.0 -> 0.37355\n",
    "# 4.0 -> 0.37146\n",
    "# 3.0 -> 0.37075\n",
    "# 2.0 -> 0.368\n",
    "# 0.5 -> 0.3606\n",
    "# 0.1 -> 0.357\n",
    "# 0.05 -> 0.35627"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
